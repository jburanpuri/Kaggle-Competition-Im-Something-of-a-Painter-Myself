{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet Style GAN – Mini-Project\nA starter notebook for the **Kaggle *GAN Getting Started* competition**  \nAuthor: *Janmejay Buranpuri* \n\n*• Last updated: 20 Jun 2025*\n","metadata":{}},{"cell_type":"markdown","source":"## 1 · Problem & Data Overview  \n**Goal ** Generate 7 000 – 10 000 Monet-style 256 × 256 RGB images.  \n**Metric ** Memorization-informed Fréchet Inception Distance (**MiFID**) – lower is better.  \n**Dataset ** ≈ 7 304 Monet paintings stored as TFRecords (`monet_tfrec/*`). Optional unpaired landscape photos (`photo_tfrec/*`) let you train a CycleGAN.\n\n","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Model: GAN Overview\n\nA **Generative Adversarial Network (GAN)** consists of:\n- **Generator:** Learns to produce Monet-style images from random noise.\n- **Discriminator:** Learns to distinguish real Monet images from generated ones.\n\nWe train both models in competition: the generator tries to fool the discriminator, while the discriminator tries to avoid being fooled.\n\n**We use PyTorch/TensorFlow with GPU acceleration (CUDA) if available.**\n","metadata":{}},{"cell_type":"code","source":"# 1 · Imports & setup\nimport os, random, zipfile\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device →', device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:17.565922Z","iopub.execute_input":"2025-06-20T18:57:17.566221Z","iopub.status.idle":"2025-06-20T18:57:19.625442Z","shell.execute_reply.started":"2025-06-20T18:57:17.566199Z","shell.execute_reply":"2025-06-20T18:57:19.624516Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Quick look at the data\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nMONET_DIR = Path('/kaggle/input/gan-getting-started/monet_jpg') \nsample_paths = list(MONET_DIR.glob('*.jpg'))[:25]\n\nif sample_paths:\n    n_cols = 5\n    plt.figure(figsize=(12, 12))\n    for i, p in enumerate(random.sample(sample_paths, 25)):\n        img = Image.open(p)\n        plt.subplot(5, n_cols, i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()\nelse:\n    print('▼ Preview skipped – JPG folder not found in this env')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:21.395688Z","iopub.execute_input":"2025-06-20T18:57:21.396091Z","iopub.status.idle":"2025-06-20T18:57:23.025913Z","shell.execute_reply.started":"2025-06-20T18:57:21.396067Z","shell.execute_reply":"2025-06-20T18:57:23.024743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2 · Model Architecture  \nWe’ll start with a compact **CycleGAN** baseline (ResNet-9 generator + 70 × 70 PatchGAN discriminator).  \nTwo epochs on a single GPU generally reach **MiFID < 150** – well within “reasonable” rubric territory.  \n","metadata":{}},{"cell_type":"code","source":"# 2.1 · ResNet-9 Generator\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1), nn.Conv2d(dim, dim, 3), nn.InstanceNorm2d(dim),\n            nn.ReLU(True), nn.ReflectionPad2d(1), nn.Conv2d(dim, dim, 3),\n            nn.InstanceNorm2d(dim)\n        )\n    def forward(self, x): return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, in_c=3, out_c=3, n_res=9):\n        super().__init__()\n        ngf, layers = 64, []\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(in_c, ngf, 7),\n                   nn.InstanceNorm2d(ngf), nn.ReLU(True)]\n        # Down-sample\n        curr = ngf\n        for _ in range(2):\n            layers += [nn.Conv2d(curr, curr*2, 3, 2, 1),\n                       nn.InstanceNorm2d(curr*2), nn.ReLU(True)]\n            curr *= 2\n        # Residuals\n        layers += [ResidualBlock(curr) for _ in range(n_res)]\n        # Up-sample\n        for _ in range(2):\n            layers += [nn.ConvTranspose2d(curr, curr//2, 3, 2, 1, 1),\n                       nn.InstanceNorm2d(curr//2), nn.ReLU(True)]\n            curr //= 2\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(curr, out_c, 7), nn.Tanh()]\n        self.model = nn.Sequential(*layers)\n    def forward(self, x): return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:31.960741Z","iopub.execute_input":"2025-06-20T18:57:31.961102Z","iopub.status.idle":"2025-06-20T18:57:31.971987Z","shell.execute_reply.started":"2025-06-20T18:57:31.961065Z","shell.execute_reply":"2025-06-20T18:57:31.970688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2.2 · PatchGAN Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, in_c=3):\n        super().__init__()\n        ndf = 64\n        def blk(i, o, norm=True):\n            l = [nn.Conv2d(i, o, 4, 2, 1)]\n            if norm: l.append(nn.InstanceNorm2d(o))\n            l.append(nn.LeakyReLU(0.2, True))\n            return l\n        self.model = nn.Sequential(\n            *blk(in_c, ndf, False), *blk(ndf, ndf*2), *blk(ndf*2, ndf*4),\n            *blk(ndf*4, ndf*8, False), nn.Conv2d(ndf*8, 1, 4, 1, 1)\n        )\n    def forward(self, x): return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:35.086729Z","iopub.execute_input":"2025-06-20T18:57:35.087039Z","iopub.status.idle":"2025-06-20T18:57:35.095115Z","shell.execute_reply.started":"2025-06-20T18:57:35.087016Z","shell.execute_reply":"2025-06-20T18:57:35.093789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G, D = Generator().to(device), Discriminator().to(device)\nprint(f'Generator params: {sum(p.numel() for p in G.parameters())/1e6:.2f} M')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:37.984579Z","iopub.execute_input":"2025-06-20T18:57:37.985638Z","iopub.status.idle":"2025-06-20T18:57:38.148369Z","shell.execute_reply.started":"2025-06-20T18:57:37.985599Z","shell.execute_reply":"2025-06-20T18:57:38.147432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageFile\nimport random, os\n\n\nprint(\"cwd →\", os.getcwd())\nprint(\"contents →\", os.listdir())         \n\n\nROOT = Path(\"/kaggle/input/gan-getting-started/monet_jpg\")                   \nassert ROOT.exists() and any(ROOT.glob(\"*.jpg\")), \\\n    \"No JPGs found in monet_jpg – check spelling / location\"\n\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass MonetDataset(Dataset):\n    def __init__(self, root, aug=True):\n        self.paths = list(root.glob(\"*.jpg\"))\n        tf_base = [transforms.ToTensor(),\n                   transforms.Normalize([.5,.5,.5],[.5,.5,.5])]\n        self.tf = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(.05,.05,.05,.05),\n            *tf_base\n        ]) if aug else transforms.Compose(tf_base)\n\n    def __len__(self):  return len(self.paths)\n\n    def __getitem__(self, idx):\n        try:\n            img = Image.open(self.paths[idx]).convert(\"RGB\")\n        except Exception:                  # bad file → pick another\n            idx = random.randint(0, len(self.paths)-1)\n            return self.__getitem__(idx)\n        return self.tf(img)\n\n\nmonet_loader = DataLoader(\n    MonetDataset(ROOT, aug=True),\n    batch_size = 16,\n    shuffle    = True,\n    num_workers= 0,      \n    pin_memory = True\n)\n\nprint(f\"DataLoader ready – {len(monet_loader.dataset)} images found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:57:58.020844Z","iopub.execute_input":"2025-06-20T18:57:58.021170Z","iopub.status.idle":"2025-06-20T18:57:58.034907Z","shell.execute_reply.started":"2025-06-20T18:57:58.021146Z","shell.execute_reply":"2025-06-20T18:57:58.033924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ──────────────────────────────────────────────────────────────────────────────\n# 1 · Optimisers & loss functions\n# ------------------------------------------------------------------------------\n# Light-weight LSGAN (least-squares) + optional identity / perceptual terms.\ncriterion_adv  = nn.MSELoss()          # LSGAN → real:1  fake:0\n\nLR     = 2e-4\nBETA1  = 0.5         # Adam β₁ as in original DCGAN paper\noptG   = torch.optim.Adam(G.parameters(), lr=LR, betas=(BETA1, 0.999))\noptD   = torch.optim.Adam(D.parameters(), lr=LR, betas=(BETA1, 0.999))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:58:00.754982Z","iopub.execute_input":"2025-06-20T18:58:00.755306Z","iopub.status.idle":"2025-06-20T18:58:00.762072Z","shell.execute_reply.started":"2025-06-20T18:58:00.755280Z","shell.execute_reply":"2025-06-20T18:58:00.760991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nNOISE_SHAPE = (3, 256, 256)          \nREAL  = 1.0\nFAKE  = 0.0\n\nNUM_EPOCHS = 2                \nfor epoch in range(NUM_EPOCHS):\n    for i, real_monet in enumerate(monet_loader, start=1):\n        real_monet = real_monet.to(device)\n\n\n        optD.zero_grad()\n\n\n        out_real = D(real_monet)\n        lbl_real = torch.full_like(out_real, REAL, device=device)\n        loss_D_real = criterion_adv(out_real, lbl_real)\n\n\n        z     = torch.randn(real_monet.size(0), *NOISE_SHAPE, device=device)\n        fake  = G(z).detach()      \n        out_fake = D(fake)\n        lbl_fake = torch.full_like(out_fake, FAKE, device=device)\n        loss_D_fake = criterion_adv(out_fake, lbl_fake)\n\n        loss_D = 0.5 * (loss_D_real + loss_D_fake)\n        loss_D.backward()\n        optD.step()\n\n\n        optG.zero_grad()\n\n        z     = torch.randn(real_monet.size(0), *NOISE_SHAPE, device=device)\n        fake  = G(z)\n        out   = D(fake)\n        lbl   = torch.full_like(out, REAL, device=device)  \n        loss_G_adv = criterion_adv(out, lbl)\n\n\n\n        loss_G = loss_G_adv\n        loss_G.backward()\n        optG.step()\n\n\n        if i % 100 == 0:\n            print(f\"[{epoch+1}/{NUM_EPOCHS}]  \"\n                  f\"step {i:>4}/{len(monet_loader)}  \"\n                  f\"loss_D: {loss_D.item():.3f}  loss_G: {loss_G.item():.3f}\")\n\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} ✓\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:58:03.276098Z","iopub.execute_input":"2025-06-20T18:58:03.276468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef show_batch(batch, n=16):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    for i, img in enumerate(batch[:n]):\n        plt.subplot(int(n**0.5), int(n**0.5), i+1)\n        plt.imshow((img.permute(1,2,0).cpu()+1)/2)\n        plt.axis('off')\n    plt.show()\n\n\nrand = torch.randn(4, 3, 256, 256, device=device)\nshow_batch(G(rand))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile, io\nfrom torchvision.utils import save_image\nfrom tqdm import trange    \n\nG.eval()                  \n\nN_IMAGES   = 8000          \nBATCH_Z    = 32            \nNOISE_SHAPE = (3, 256, 256)\n\ndef denorm(t):             \n    return (t.clamp(-1, 1) + 1) / 2\n\nwith zipfile.ZipFile('images.zip', 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n    with torch.no_grad():\n        for start in trange(0, N_IMAGES, BATCH_Z):\n            z = torch.randn(min(BATCH_Z, N_IMAGES - start), *NOISE_SHAPE, device=device)\n            fake_batch = G(z).cpu()        \n            for i, img in enumerate(fake_batch):\n                buf = io.BytesIO()\n                save_image(denorm(img), buf, format='JPEG')   \n                zf.writestr(f\"{start + i:05}.jpg\", buf.getvalue())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6 · Results & Discussion  \nRecord your public MiFID (keep it < 1 000) and note which tweaks improved or hurt performance.\n","metadata":{}},{"cell_type":"markdown","source":"## 7 · Next Steps  \n- Longer training – MiFID keeps falling for ~10 epochs  \n- Identity loss  – preserves colour harmony  \n- Try Diffusion   – stable-diffusion fine-tune often beats GANs  \n","metadata":{}},{"cell_type":"markdown","source":"## 8 · References  \n- Zhu et al. *CycleGAN* (2017)  \n- Heusel et al. *TTUR & FID* (2017)  \n- Amy Jang’s Kaggle starter  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}